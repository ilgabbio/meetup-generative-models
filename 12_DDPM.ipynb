{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8d267d",
   "metadata": {},
   "source": [
    "# GAN architectures\n",
    "\n",
    "<img src=\"images/GANsnRoses.png\" width=\"150pt\"/>\n",
    "\n",
    "Many GANs in our armory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a9d8a",
   "metadata": {},
   "source": [
    "## [Denoising Diffusion Probabilistic Models (DDPMs)](https://arxiv.org/abs/2006.11239) (Jun 2020)\n",
    "\n",
    "Idea: __sampling a simple distribution__ (gaussian) and __moving on a path__ toward the data distribution. \n",
    "\n",
    "<img src=\"images/DDPM_path.png\" width=\"750pt\"/>\n",
    "\n",
    "Coinsider a DAE as a _denoising function_, imagine to iterate the process:\n",
    "\n",
    "<img src=\"images/DDPM.png\" width=\"750pt\"/>\n",
    "\n",
    "Consider the denoising process a Markov chain with:\n",
    "\n",
    "$$\\large\n",
    "p(x_{0..T}) = p(x_T)\\prod_{t=0}^T p(x_{t-1}|x_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\large\n",
    "p(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu(x_t,t), \\Sigma(x_t,t))\n",
    "$$\n",
    "\n",
    "The opposite (diffusion) process is:\n",
    "\n",
    "$$\\large\n",
    "p(x_{1..T}|x_0) = \\prod_{t=1}^T q(x_{t}|x_{t-1})\n",
    "$$\n",
    "\n",
    "where for small steps it is assumed that:\n",
    "\n",
    "$$\\large\n",
    "q(x_{t}|x_{t-1}) = \\mathcal{N}(x_{t}; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\n",
    "$$\n",
    "\n",
    "For a variance schedule $\\beta_1, \\cdots, \\beta_T$.\n",
    "\n",
    "### The loss\n",
    "\n",
    "Combination of forward and backward steps ($p$ and $q$) in __one denoising step is a VAE__.\n",
    "\n",
    "Objective, __maximization of the ELBO__:\n",
    "\n",
    "$$\\large\n",
    "\\mathcal{L} = \\mathbb{E}_q\\left[\\log p(x_T) + \\sum_{t\\geq1}\\log\\frac{p(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\\right]\n",
    "$$\n",
    "\n",
    "### The architecture\n",
    "\n",
    "<img src=\"images/DDPM_arch.png\" width=\"750pt\"/>\n",
    "\n",
    "### Some qualitative results\n",
    "\n",
    "Examples of denoising/diffusion:\n",
    "\n",
    "<img src=\"images/DDPM_examples.png\" width=\"750pt\"/>\n",
    "\n",
    "On the MNIST dataset:\n",
    "\n",
    "<img src=\"images/DDPM_mnist.gif\" width=\"500pt\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed44ca",
   "metadata": {},
   "source": [
    "## [Diffusion models Beat GANs](https://arxiv.org/abs/2105.05233) (May 2021)\n",
    "\n",
    "After many improvements to DDPM:\n",
    "\n",
    "<img src=\"images/DDPM2_examples.png\" width=\"750pt\"/>\n",
    "\n",
    "### List of main changes\n",
    "\n",
    "- conditional denoising:\n",
    " - classifier incorporated in the model;\n",
    " - gradient scaling to better guide to the required class;\n",
    "- AdaGN:\n",
    " - $AdaGN(h,y) = y_s GroupNorm(h) + y_b$;\n",
    " - $y = [y_s,y_s]$ as linear projection of $t$ and class embedding;\n",
    " - [group normalization](https://arxiv.org/abs/1803.08494) acts on __contiguous groups of channels__;\n",
    "- adaptive variance schedule via NN;\n",
    "- [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/abs/2010.02502);\n",
    "- other ablations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
